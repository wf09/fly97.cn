<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">
  <meta name="google-site-verification" content="8asjqKj8nljgUycZFS4L-H9Ivw5U1DHaXAOrHxxnRUg">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="//cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.14.0/css/all.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/animate.css@3.1.1/animate.min.css">
  <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/pace-js@1.0.2/themes/blue/pace-theme-minimal.css">
  <script src="//cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>

<script class="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.fly97.cn","root":"/","scheme":"Mist","version":"8.0.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":true,"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"prism":false,"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"search.xml","localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false}};
  </script>

  <meta name="description" content="本文介绍了 PyTorch 的自动求导机制和计算图。 摘要：深度学习的算法本质是通过反向传播求导数，PyTorch 的 Autograd模块实现了此功能。在 Tensor 上的所有操作，Autograd都能为他们自动提供微分，避免手动求导的复杂过程。 关键字：PyTorch，自动求导，计算图">
<meta property="og:type" content="article">
<meta property="og:title" content="PyTorch中的自动求导机制和计算图">
<meta property="og:url" content="https://www.fly97.cn/p/automatic-derivation-mechanism-and-calculation-graph-in-PyTorch/index.html">
<meta property="og:site_name" content="个人随想">
<meta property="og:description" content="本文介绍了 PyTorch 的自动求导机制和计算图。 摘要：深度学习的算法本质是通过反向传播求导数，PyTorch 的 Autograd模块实现了此功能。在 Tensor 上的所有操作，Autograd都能为他们自动提供微分，避免手动求导的复杂过程。 关键字：PyTorch，自动求导，计算图">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://ftp.fly97.cn/image/image-20200820171041627.png">
<meta property="og:image" content="https://ftp.fly97.cn/image/image-20200820172727826.png">
<meta property="article:published_time" content="2020-08-18T05:27:00.000Z">
<meta property="article:modified_time" content="2020-08-18T05:27:00.000Z">
<meta property="article:author" content="个人随想">
<meta property="article:tag" content="PyTorch">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ftp.fly97.cn/image/image-20200820171041627.png">


<link rel="canonical" href="https://www.fly97.cn/p/automatic-derivation-mechanism-and-calculation-graph-in-PyTorch/">


<script class="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>PyTorch中的自动求导机制和计算图 | 个人随想</title>
  


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?b6247b5cec5ce45c64bcc9851a8ee960";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  body { margin-top: 2rem; }

  .use-motion .menu-item,
  .use-motion .sidebar,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header {
    visibility: visible;
  }

  .use-motion .header,
  .use-motion .site-brand-container .toggle,
  .use-motion .footer { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle,
  .use-motion .custom-logo-image {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line {
    transform: scaleX(1);
  }

  .search-pop-overlay, .sidebar-nav { display: none; }
  .sidebar-panel { display: block; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="个人随想" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="headband"></div>

  <main class="main">
    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <h1 class="site-title">个人随想</h1>
      <i class="logo-line"></i>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>站点地图</a>

  </li>
        <li class="menu-item menu-item-book">

    <a href="/book" rel="section"><i class="fa fa-book fa-fw"></i>书籍</a>

  </li>
        <li class="menu-item menu-item-download">

    <a href="/download" rel="section"><i class="fa fa-download fa-fw"></i>下载</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off" maxlength="80"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div class="search-result-container no-result">
  <div class="search-result-icon">
    <i class="fa fa-spinner fa-pulse fa-5x"></i>
  </div>
</div>

    </div>
  </div>

</div>
        
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>

  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <section class="post-toc-wrap sidebar-panel">
          <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%87%AA%E5%8A%A8%E6%B1%82%E5%AF%BC%E6%9C%BA%E5%88%B6%E5%AE%9E%E4%BE%8B"><span class="nav-number">1.</span> <span class="nav-text">自动求导机制实例</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A2%AF%E5%BA%A6%E5%87%BD%E6%95%B0%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="nav-number">2.</span> <span class="nav-text">梯度函数的使用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE%E7%9A%84%E6%9E%84%E5%BB%BA%E7%9A%84%E5%90%AF%E7%94%A8%E5%92%8C%E7%A6%81%E7%94%A8"><span class="nav-number">3.</span> <span class="nav-text">计算图的构建的启用和禁用</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%8E%E8%AE%B0"><span class="nav-number">4.</span> <span class="nav-text">后记</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Tensor"><span class="nav-number">4.1.</span> <span class="nav-text">Tensor</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#torch-autograd-backward"><span class="nav-number">4.2.</span> <span class="nav-text">torch.autograd.backward</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%BC%E6%95%B0%E8%BF%90%E7%AE%97"><span class="nav-number">4.3.</span> <span class="nav-text">导数运算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%AE%A1%E7%AE%97%E5%9B%BE"><span class="nav-number">4.4.</span> <span class="nav-text">计算图</span></a></li></ol></li></ol></div>
      </section>
      <!--/noindex-->

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">个人随想</p>
  <div class="site-description" itemprop="description">你不是我 怎么可能做到感同身受</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">51</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">41</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/wf09" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;wf09" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:universitypking@gmail.com" title="E-Mail → mailto:universitypking@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>


  <div class="links-of-blogroll animated">
    <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="https://fly97.cn/" title="https:&#x2F;&#x2F;fly97.cn" rel="noopener" target="_blank">个人随想</a>
        </li>
    </ul>
  </div>

      </section>
    </div>
  </aside>
  <div class="sidebar-dimmer"></div>


    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>

  <a href="https://github.com/wf09" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


    <div class="main-inner post posts-expand">
      

      

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://www.fly97.cn/p/automatic-derivation-mechanism-and-calculation-graph-in-PyTorch/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="个人随想">
      <meta itemprop="description" content="你不是我 怎么可能做到感同身受">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="个人随想">
    </span>

    
    
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          PyTorch中的自动求导机制和计算图
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2020-08-18 13:27:00" itemprop="dateCreated datePublished" datetime="2020-08-18T13:27:00+08:00">2020-08-18</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">
        <p>本文介绍了 PyTorch 的自动求导机制和计算图。</p>
<p><strong>摘要</strong>：深度学习的算法本质是通过反向传播求导数，PyTorch 的 <code>Autograd</code>模块实现了此功能。在 Tensor 上的所有操作，<code>Autograd</code>都能为他们自动提供微分，避免手动求导的复杂过程。</p>
<p><strong>关键字</strong>：PyTorch，自动求导，计算图</p>
<a id="more"></a>

<p>PyTorch会根据计算过程来自动生成动态图，然后根据动态图的创建过程进行反向传播，计算得到每个节点的梯度值。为了能够记录张量的梯度，首先需要在创建张量的时候设置一个参数<code>requires_grad=True</code>，意味着这个张量将会加入到计算图中，作为计算图的叶子节点参与计算通过一系列的计算最后输出结果张量，也就是根节点。几乎所有的张量创建方式都可以指定这个参数，一旦指定了这个参数，在后续的计算中得到的中间结果的张量都会被设置成<code>requires_grad=True</code>。对于PyTorch 来说，每一个张量都有一个<code>grad_fn</code>方法，这个方法包含创建该张量的运算的导数信息。在反向传播过程中，通过传入后一层的神经网络的梯度，该函数会计算出参与运算的所有张量的梯度，<code>grad_fn</code>本身也携带着计算图的信息，该方法本身有一个<code>next_functions</code>属性，包含连接该张量的其他张量的<code>grad_fn</code>。通过不断反向传播回溯中间张量的计算节点，可以得到所有张量的梯度。一个张量的梯度张量的信息保存在该张量的grad属性中。</p>
<p>除 PyTorch 张量本身外，PyTorch提供了一个专门用来自动求导的包，即<code>torch.autograd</code>.它包含了两个重要的函数，即<code>torch.autograd.backward</code>函数和<code>torch.autograd.grad</code>函数。</p>
<p><code>torch.autograd.backward</code>函数通过传入根节点张量，以及起始梯度张量(形状和当前张量的相同)，可以计算产生该根节点所有对应的叶子节点的梯度。当张量为标量张量(即只有一个元素的张量)时，可以不传入起始梯度张量，默认会设置初始梯度张量为1。当计算梯度张量时，原来建立起来的计算图会自动被释放，如果需要再次做自动求导，因为计算图会被自动释放，如果需要再次做自动求导，因为计算图已经不存在，就会报错。如果要在反向传播的时候保留计算图，可以设置<code>retain_graph=True</code>。另外，在自动求导的时候默认不会建立反向传播的计算图(反向传播也是一个计算过程，可以动态创建计算图)，如果需要在反向传播计算的同时建立和梯度张量有关的计算图(在某些情况下，如需要计算高阶导数的情况下，不过这种情况比较少)，可以设置<code>creat_graph=True</code>。对于一个可求导的张量，也可以直接调用该张量内部的<code>backward</code>方法来进行自动求导。</p>
<h2 id="自动求导机制实例"><a href="#自动求导机制实例" class="headerlink" title="自动求导机制实例"></a>自动求导机制实例</h2><p>下面举一个简单的例子来说明自动求导是如何使用的。根据高等数学的知识可知，若定义一个函数$f(x)=x^2$，则它的导数$f(x)=2x$。于是可以创建一个可求导的张量来测试具体的导数。具体如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">t1 = t.randn(<span class="number">3</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">t1</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[-0.6704,  1.1710,  0.7608],</span></span><br><span class="line"><span class="string">        [ 1.2378, -0.5393, -0.9865],</span></span><br><span class="line"><span class="string">        [ 0.2863,  0.5295, -0.4555]], requires_grad=True)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">t2 = t1.pow(<span class="number">2</span>).sum() </span><br><span class="line">t2.backward(retain_graph=<span class="literal">True</span>) <span class="comment">#梯度反向传播</span></span><br><span class="line">t1.grad <span class="comment">#梯度是张量原始分量的2倍</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;tensor([[-1.3407,  2.3419,  1.5216],</span></span><br><span class="line"><span class="string">        [ 2.4756, -1.0786, -1.9729],</span></span><br><span class="line"><span class="string">        [ 0.5725,  1.0589, -0.9110]]</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">t2 = t1.pow(<span class="number">2</span>).sum()  <span class="comment">#再次计算张量的所有分量平方和</span></span><br><span class="line">t2.backward() <span class="comment"># 梯度再次反向传播</span></span><br><span class="line">t1.grad <span class="comment">#梯度累积</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[-2.1156, -1.1415,  7.4562],</span></span><br><span class="line"><span class="string">        [ 0.0900, -4.8776, -0.5413],</span></span><br><span class="line"><span class="string">        [-8.0727, -0.1184, -6.8779]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">t1.grad.zero_() <span class="comment">#梯度清零</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>需要注意的一点是，张量绑定的梯度张量在不清空的情况下会逐渐累积。这种特性在某些情况下是有用的，比如，需要一次性求很多迷你批次的累计梯度，但在一般情况下，不需要用到这个特性，所以要注意将张量的梯度清零。</p>
<h2 id="梯度函数的使用"><a href="#梯度函数的使用" class="headerlink" title="梯度函数的使用"></a>梯度函数的使用</h2><p>在某些情况下，不需要求出当前张量对所有产生该张量的叶子节点的梯度，此时可以用<code>torch.autograd.grad</code>函数，这个函数的参数是两个张量，第一个张量是计算图的数据结果张量(或是张量列表)，第二个张量是需要对计算图求导的张量(或张量列表)。最后输出的结果是第一个张量对第二个张量求导的结果(注意梯度会累积，和前面介绍的<code>torch.autograd.backward</code>函数的行为一样)。<strong>需要注意的是</strong>，这个函数不会改变叶子节点的grad属性。而函数<code>torch.autograd.backward</code>会设置叶子节点的grad属性为最后求出梯度张量。<code>torch.autograd.grad</code>会在反向传播求导时释放计算图，如果需要保留计算图，同样可以设置<code>retain_graph=True</code>.如果需要反向传播的计算图，可以设置<code>create_graph=True</code>.</p>
<p>另外，有时候会碰到一种情况是求导的两个张量之间在计算图上没有关联，在这种情况下函数会报错，如果不需要函数的报错行为，可以设置allow_unused=True这个参数，结果会返回分量全为0的梯度张量(因为两个张量没有关联，所以求导的梯度为0).</p>
<p>具体的使用方法可以参考以下代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">t1 = t.randn(<span class="number">3</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>) <span class="comment">#初始化t1张量</span></span><br><span class="line">t1</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[-0.1082, -1.0099, -0.4560],</span></span><br><span class="line"><span class="string">        [-0.3910, -0.9767,  0.6419],</span></span><br><span class="line"><span class="string">        [ 1.1544,  0.3572, -1.3304]], requires_grad=True)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line">t2 = t1.pow(<span class="number">2</span>).sum() <span class="comment">#根据t1张量求t2张量</span></span><br><span class="line">t.autograd.grad(t2, t1) <span class="comment"># t2张量对t1张量求导</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(tensor([[-0.2165, -2.0197, -0.9120],</span></span><br><span class="line"><span class="string">         [-0.7820, -1.9535,  1.2837],</span></span><br><span class="line"><span class="string">         [ 2.3088,  0.7145, -2.6608]]),)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<h2 id="计算图的构建的启用和禁用"><a href="#计算图的构建的启用和禁用" class="headerlink" title="计算图的构建的启用和禁用"></a>计算图的构建的启用和禁用</h2><p>由于计算图的构建需要消耗内存和计算资源，在一些情况下，计算图并不是必要的，比如<strong>神经网络的推导</strong>。在这种情况下，可以使用<code>torch.no_grad</code>上下文管理器，在这个上下文管理器的作用域内进行的神经网络计算不会构建任何计算图。</p>
<p>另外，还有一种情况是对于一个张量，我们在反向传播的时候可能不需要让梯度通过这个张量的节点，也就是新建的计算图要和原来的计算图分离。在这种情况下，可以使用张量的<code>detach</code>方法，通过调用这个方法，可以返回一个新的张量，该张量会成为一个新的计算图的叶子节点，新的计算图和老的计算图互相分离，互不影响。具体代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">t1 = t.randn(<span class="number">3</span>, <span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line">t2 = t1.sum()</span><br><span class="line"></span><br><span class="line">t2   <span class="comment">#t2的计算构建了计算图，输出结果带有grad_fn</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> tensor(2.2761, grad_fn=&lt;SumBackward0&gt;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> t.no_grad():</span><br><span class="line">    t3 = t1.sum()</span><br><span class="line">    </span><br><span class="line">t3  <span class="comment">#t3的计算没有构建计算图，输出结果没有grad_fn</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor(2.2761)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">t1.sum()   <span class="comment">#保持原来的计算图</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor(2.2761, grad_fn=&lt;SumBackward0&gt;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">t1.sum().detach()</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string"> tensor(2.2761)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h2 id="后记"><a href="#后记" class="headerlink" title="后记"></a>后记</h2><h3 id="Tensor"><a href="#Tensor" class="headerlink" title="Tensor"></a>Tensor</h3><p>如果我们需要计算某个Tensor的导数，那么我们需要设置其<code>requires_grad=True</code>.</p>
<p>Tensor包含以下属性：</p>
<ul>
<li><code>grad</code>：保存tensor的梯度，形状与Tensor一致。每次在计算backward时都需要将前一时刻的梯度归零，否则梯度值会一直累加。</li>
<li><code>grad_fn</code>：指向一个Function，记录Tensor的操作历史，即它是什么操作的输出，用来构建计算图。如果某一个变量是由用户创建的，则他为<strong>叶子节点</strong>，对应的<code>grad_fn</code>等于<code>None</code>。只有<strong>根节点</strong>的<code>grad_fn</code>才有效，用于指示梯度函数是哪种类型。</li>
<li><code>is_leaf</code>: 用来指示该Tensor是否是叶子节点。</li>
<li><code>requires_grad</code>: 设置为<code>True</code>则表示该Tensor需要求导</li>
</ul>
<p>通过几个例子来了解一下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">a = t.ones(<span class="number">3</span>, <span class="number">4</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">a</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.]], requires_grad=True)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">b = t.zeros(<span class="number">3</span>, <span class="number">4</span>)</span><br><span class="line">b</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0.],</span></span><br><span class="line"><span class="string">        [0., 0., 0., 0.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 张量加法</span></span><br><span class="line"></span><br><span class="line">c = a + b</span><br><span class="line">c</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 或</span></span><br><span class="line">c = a.add(b)</span><br><span class="line">c</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">d = c.sum()</span><br><span class="line">d.backward() <span class="comment"># 反向传播</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意两者的区别</span></span><br><span class="line">c.data.sum(), c.sum()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">(tensor(12.), tensor(12., grad_fn=&lt;SumBackward0&gt;))</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">a.grad</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.],</span></span><br><span class="line"><span class="string">        [1., 1., 1., 1.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># 此处虽然没有指定c需要求导，但c依赖于a，a需要求导</span></span><br><span class="line"><span class="comment"># 因此c的requires_grad属性会自动设为True</span></span><br><span class="line">a.requires_grad, b.requires_grad, c.requires_grad</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;(True, False, True)&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 由用户创建的 tensor 属于叶子节点，对应的grad_fn是None</span></span><br><span class="line">a.is_leaf, b.is_leaf, c.is_leaf</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;(True, True, False)&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># c.grad是None, c不是叶子节点, 他的梯度是用来计算a的梯度</span></span><br><span class="line"><span class="comment"># 虽然c.requires_grad = True, 但其梯度计算完了即被释放</span></span><br><span class="line"></span><br><span class="line">c.grad <span class="keyword">is</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;True&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>



<h3 id="torch-autograd-backward"><a href="#torch-autograd-backward" class="headerlink" title="torch.autograd.backward"></a>torch.autograd.backward</h3><p>先看一下backward的接口是如何定义的：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">torch.autograd.backward(</span><br><span class="line">		tensors, </span><br><span class="line">		grad_tensors&#x3D;None, </span><br><span class="line">		retain_graph&#x3D;None, </span><br><span class="line">		create_graph&#x3D;False, </span><br><span class="line">		grad_variables&#x3D;None)</span><br></pre></td></tr></table></figure>

<ul>
<li><code>tensor</code>: 用于计算梯度的tensor。也就是说这两种方式是等价的：<code>torch.autograd.backward(z) == z.backward()</code></li>
<li><code>grad_tensors</code>: 在计算矩阵的梯度时会用到。他其实也是一个tensor，shape一般需要和前面的<code>tensor</code>保持一致。</li>
<li><code>retain_graph</code>: 通常在调用一次backward后，PyTorch会自动把计算图销毁，所以要想对某个变量重复调用backward，则需要将该参数设置为<code>True</code></li>
<li><code>create_graph</code>: 当设置为<code>True</code>的时候可以用来计算更高阶的梯度</li>
<li><code>grad_variables</code>: 这个官方说法是grad_variables’ is deprecated. Use ‘grad_tensors’ instead.也就是说这个参数后面版本中应该会丢弃，直接使用<code>grad_tensors</code>.</li>
</ul>
<p>使用以下代码尝试解释<code>grad_tensors</code>的作用。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.ones(<span class="number">2</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = x + <span class="number">2</span></span><br><span class="line">z.backward()</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">RuntimeError: grad can be implicitly created only for scalar outputs</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>当代码被运行，会有一个上述的<code>RuntimeError</code>被抛出。上面的报错信息意思是<strong>只有对标量输出它才会计算梯度</strong>，而求一个矩阵对令一个矩阵的导数束手无策。<br>$$<br>X=[x_0\quad x_1]\quad  Z=X+2=[x_0+2\quad x_1+2]\Rightarrow \frac{\partial Z}{\partial X}=?<br>$$<br>那么我们只要相办法把矩阵转变成一个标量不就好了？比如我们可以对<code>Z</code>求和，然后用求和得到的标量在对x求导，这样不会对结果有影响。即：<br>$$<br>Z_{sum}=\sum{z_i}=x_0+x_1+8\quad then \quad\frac{\partial Z_{sum}}{\partial X_0}=\frac{\partial Z_{sum}}{\partial X_1}=1<br>$$<br>我们可以看到对z求和以后再计算梯度没有报错，结果也和预期一样：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"></span><br><span class="line">x = t.ones(<span class="number">2</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = x + <span class="number">2</span></span><br><span class="line"></span><br><span class="line">z.sum().backward()</span><br><span class="line">x.grad</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([1., 1.])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>我们再仔细想想，对z求和不就是等价于z<strong>点乘一个一样维度的全为1的矩阵</strong>吗？即$sum(Z)=dot(Z,I)$,而这个I也就是我们需要传入的<code>grad_tensor</code>参数。点乘只是对一维向量而言的，对于矩阵或更高维的张量，可以看作是<strong>对每一个维度做点乘</strong>。</p>
<p>代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line">x = t.ones(<span class="number">2</span>,requires_grad=<span class="literal">True</span>)</span><br><span class="line">z = x + <span class="number">2</span></span><br><span class="line"></span><br><span class="line">z.backward(t.ones_like(z))</span><br><span class="line">x.grad</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([1., 1.])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br></pre></td></tr></table></figure>

<p>弄个再复杂一点的：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">x = t.tensor([[<span class="number">2.</span>, <span class="number">1.</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line">y = t.tensor([[<span class="number">1.</span>, <span class="number">2.</span>], [<span class="number">3.</span>, <span class="number">4.</span>]], requires_grad=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">z = t.mm(x, y)</span><br><span class="line">print(<span class="string">f&quot;z:<span class="subst">&#123;z&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">z.backward(t.tensor([[<span class="number">1.</span>, <span class="number">0</span>]]), retain_graph=<span class="literal">True</span>)</span><br><span class="line">print(<span class="string">f&quot;x.grad:<span class="subst">&#123;x.grad&#125;</span>&quot;</span>)</span><br><span class="line">print(<span class="string">f&quot;y.grad:<span class="subst">&#123;y.grad&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">z:tensor([[5., 8.]], grad_fn=&lt;MmBackward&gt;)</span></span><br><span class="line"><span class="string">x.grad:tensor([[1., 3.]])</span></span><br><span class="line"><span class="string">y.grad:tensor([[2., 0.],</span></span><br><span class="line"><span class="string">        [1., 0.]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>总结：说了这么多，grad_tensors的作用其实可以简单地理解成在求梯度时候地权重，因为可能梯度对影响结果程度不同。</p>
<p><strong>知乎高赞评论：</strong>假设是在z点backward，输入<code>grad_tensors</code>应该是目标函数(scalar)f对z的梯度，那么<br>$$<br>\frac{\partial f}{\partial X}=\frac{\partial f}{\partial z} \times \frac{\partial z}{\partial x}<br>$$<br>其中，传入的第一项就是传入<code>grad_tensors</code></p>
<h3 id="导数运算"><a href="#导数运算" class="headerlink" title="导数运算"></a>导数运算</h3><p>接着我们来看看autograd计算的导数和我们手动推导的导数的区别。</p>
<p>给出下列函数表达式：<br>$$<br>y=x^2e^x<br>$$<br>他的导函数是：<br>$$<br>\frac{\partial y}{\partial x}=2xe^x+x^2e^x<br>$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch <span class="keyword">as</span> t</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">f</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;计算y&quot;&quot;&quot;</span></span><br><span class="line">    y = x**<span class="number">2</span> * t.exp(x)</span><br><span class="line">    <span class="keyword">return</span> y</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gradf</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;手动求导函数&quot;&quot;&quot;</span></span><br><span class="line">    dx = <span class="number">2</span>*x*t.exp(x) + x**<span class="number">2</span>*t.exp(x)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dx</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置随机数种子, 使结果可以复现</span></span><br><span class="line">t.manual_seed(<span class="number">0</span>)</span><br><span class="line">np.random.seed(<span class="number">0</span>)</span><br><span class="line">x = t.randn(<span class="number">3</span>, <span class="number">4</span>, requires_grad = <span class="literal">True</span>)</span><br><span class="line">y = f(x)</span><br><span class="line">y</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[11.0879,  0.0642,  0.5373,  0.5705],</span></span><br><span class="line"><span class="string">        [ 0.3976,  0.4830,  0.2435,  1.6235],</span></span><br><span class="line"><span class="string">        [ 0.2520,  0.1087,  0.1960,  0.0398]], grad_fn=&lt;MulBackward0&gt;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line">y.backward(t.ones(y.size())) <span class="comment"># 和grad_tensor形状一致</span></span><br><span class="line">x.grad</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[25.4785, -0.3734,  0.0441,  2.5776],</span></span><br><span class="line"><span class="string">        [-0.3356, -0.2077,  1.4510,  5.4982],</span></span><br><span class="line"><span class="string">        [-0.4487, -0.4302, -0.4611,  0.4765]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">y.sum().backward()</span><br><span class="line">x.grad</span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[25.4785, -0.3734,  0.0441,  2.5776],</span></span><br><span class="line"><span class="string">        [-0.3356, -0.2077,  1.4510,  5.4982],</span></span><br><span class="line"><span class="string">        [-0.4487, -0.4302, -0.4611,  0.4765]])</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="comment"># autograd的计算结果与利用公式手动计算的结果一致</span></span><br><span class="line">gradf(x)</span><br><span class="line"></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">tensor([[25.4785, -0.3734,  0.0441,  2.5776],</span></span><br><span class="line"><span class="string">        [-0.3356, -0.2077,  1.4510,  5.4982],</span></span><br><span class="line"><span class="string">        [-0.4487, -0.4302, -0.4611,  0.4765]], grad_fn=&lt;AddBackward0&gt;)</span></span><br><span class="line"><span class="string">&quot;&quot;&quot;</span></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h3 id="计算图"><a href="#计算图" class="headerlink" title="计算图"></a>计算图</h3><p>PyTorch 中的 autograd 的底层采用了计算图，计算图是一种<strong>特殊的有向无环图(DAG)</strong>.用于记录算子和变量之间的关系。一般用矩形表示算子，椭圆形表示变量。如表达式$z=wx+b$可以分解为$y=wx$和$z=y+b$，其计算图如下图所示，图中的<code>MUL</code>和<code>ADD</code>都是算子，$w、x、b$为变量。</p>
<img src="https://ftp.fly97.cn/image/image-20200820171041627.png" alt="image-20200820171041627" style="zoom:50%;" />

<p>如上有向无环图中，$X$和$b$是叶子节点(leaf node)，这些节点通常由用户自己创建，不依赖其他变量。$z$称为根节点，是计算图的最终目标。利用链式法则很容易求得各个叶子节点的梯度。<br>$$<br>\frac{\partial z}{\partial b}=1\quad\frac{\partial z}{\partial y}=1<br>$$</p>
<p>$$<br>\frac{\partial y}{\partial w}=x\quad\frac{\partial y}{\partial x}=w<br>$$</p>
<p>$$<br>\frac{\partial z}{\partial x}=\frac{\partial z}{\partial y} \times\frac{\partial y}{\partial x}=1\times w<br>$$</p>
<p>$$<br>\frac{\partial z}{\partial w}=\frac{\partial z}{\partial y} \times\frac{\partial y}{\partial w}=1\times x<br>$$</p>
<p>而有了计算图，上述链式求导即可利用计算图的反向传播自动完成，其传播过程如下图所示。</p>
<img src="https://ftp.fly97.cn/image/image-20200820172727826.png" alt="image-20200820172727826" style="zoom:50%;" />

<p>在 PyTorch 实现中，<code>autograd</code>会随着用户的操作，记录生成当前 <strong>Tensor</strong> 的所有操作，并由此建立一个有向无环图。用户没进行一个操作，相应的计算图就会发生改变。更底层的实现中，图中记录了操作<code>Function</code>，每个变量在图中的位置可通过其<code>grad_fn</code>属性在图中的位置可以推测得到。在反向传播过程中，<code>autograd</code>沿着这个图从当前变量(根节点$z$)溯源，可以利用链式求导法则计算所有叶子节点的梯度。每一个前向传播操作的函数都有与之对应的反向传播函数用来计算输入的各个Tensor的梯度，这些函数的函数名通常以<code>Backward</code>结尾。</p>
<p>部分转载自：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/83172023">https://zhuanlan.zhihu.com/p/83172023</a></p>

    </div>

    
    
    

    <footer class="post-footer">
          

  <div class="followme">
    <span>欢迎关注我的其它发布渠道</span>

    <div class="social-list">

        <div class="social-item">
          <a target="_blank" class="social-link" href="/atom.xml">
            <span class="icon">
              <i class="fa fa-rss"></i>
            </span>

            <span class="label">RSS</span>
          </a>
        </div>
    </div>
  </div>

          <div class="post-tags">
              <a href="/tags/PyTorch/" rel="tag"># PyTorch</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/p/Tensor-in-PyTorch/" rel="prev" title="PyTorch中的Tensor">
                  <i class="fa fa-chevron-left"></i> PyTorch中的Tensor
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/p/loss-function-and-optimizer-in-PyTorch/" rel="next" title="PyTorch中的损失函数和优化器">
                  PyTorch中的损失函数和优化器 <i class="fa fa-chevron-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
  
  
  



      

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      const activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      const commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

    </div>
  </main>

  <footer class="footer">
    <div class="footer-inner">
      

      
  <div class="beian"><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">鲁ICP备19008796号-3 </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">个人随想</span>
</div>

    </div>
  </footer>

  
  <script src="//cdn.jsdelivr.net/npm/animejs@3.2.0/lib/anime.min.js"></script>
<script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  




  <script src="/js/local-search.js"></script>





  <script src="//code.tidio.co/fhmpsewhwzms2jcqfmhhn0r7drcjs0oy.js"></script>



<script>
document.querySelectorAll('.pdfobject-container').forEach(element => {
  const url = element.dataset.target;
  const pdfOpenParams = {
    navpanes : 0,
    toolbar  : 0,
    statusbar: 0,
    pagemode : 'thumbs',
    view     : 'FitH'
  };
  const pdfOpenFragment = '#' + Object.entries(pdfOpenParams).map(([key, value]) => `${key}=${encodeURIComponent(value)}`).join('&');
  const fullURL = `/lib/pdf/web/viewer.html?file=${encodeURIComponent(url)}${pdfOpenFragment}`;

  if (NexT.utils.supportsPDFs()) {
    element.innerHTML = `<embed class="pdfobject" src="${url + pdfOpenFragment}" type="application/pdf" style="height: ${element.dataset.height};">`;
  } else {
    element.innerHTML = `<iframe src="${fullURL}" style="height: ${element.dataset.height};" frameborder="0"></iframe>`;
  }
});
</script>




  








  

  
      <script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              const target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    const script = document.createElement('script');
    script.src = '//cdn.jsdelivr.net/npm/mathjax@3.1.0/es5/tex-mml-chtml.js';
    script.defer = true;
    document.head.appendChild(script);
  } else {
    MathJax.startup.document.state(0);
    MathJax.typesetClear();
    MathJax.texReset();
    MathJax.typeset();
  }
</script><!-- hexo-inject:begin --><!-- hexo-inject:end -->

    

  

</body>
</html>
